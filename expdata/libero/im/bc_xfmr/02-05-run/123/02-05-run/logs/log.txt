
============= Initialized Observation Utils with Obs Spec =============

using obs modality: low_dim with keys: ['gripper_states', 'ee_ori', 'ee_pos', 'ee_states', 'joint_states']
using obs modality: rgb with keys: ['eye_in_hand_rgb', 'agentview_rgb']
using obs modality: depth with keys: []
using obs modality: scan with keys: []

============= Loaded Environment Metadata =============
/home/jacob/projects/strap_project/STRAP/data/retrieval_results/put_both_moka_pots_retrieved_dataset.hdf5
obs key agentview_rgb with shape (128, 128, 3)
obs key ee_ori with shape (3,)
obs key ee_pos with shape (3,)
obs key ee_states with shape (6,)
obs key eye_in_hand_rgb with shape (128, 128, 3)
obs key gripper_states with shape (2,)
obs key joint_states with shape (7,)

ObservationKeyToModalityDict: lang_emb not found, adding lang_emb to mapping with assumed low_dim modality!
/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Created GPT_Backbone model with number of parameters: 6312448
Initialized model with 34868619 parameters

============= Model Summary =============
ObservationKeyToModalityDict: mean not found, adding mean to mapping with assumed low_dim modality!
ObservationKeyToModalityDict: scale not found, adding scale to mapping with assumed low_dim modality!
ObservationKeyToModalityDict: logits not found, adding logits to mapping with assumed low_dim modality!
BC_Transformer_GMM (
  ModuleDict(
    (policy): TransformerGMMActorNetwork(
        action_dim=7, std_activation=softplus, low_noise_eval=True, num_nodes=5, min_std=0.005
  
        encoder=ObservationGroupEncoder(
            group=obs
            ObservationEncoder(
                Key(
                    name=agentview_rgb
                    shape=[3, 128, 128]
                    modality=rgb
                    randomizer=ModuleList(
                      (0): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)
                    )
                    net=VisualCoreLanguageConditioned(
                      input_shape=[3, 116, 116]
                      output_shape=[64]
                      backbone_net=ResNet18ConvFiLM(input_channel=3, input_coord_conv=False)
                      pool_net=SpatialSoftmax(num_kp=32, temperature=1.0, noise=0.0)
                    )
                    sharing_from=None
                )
                Key(
                    name=ee_ori
                    shape=[3]
                    modality=low_dim
                    randomizer=ModuleList(
                      (0): None
                    )
                    net=None
                    sharing_from=None
                )
                Key(
                    name=ee_pos
                    shape=[3]
                    modality=low_dim
                    randomizer=ModuleList(
                      (0): None
                    )
                    net=None
                    sharing_from=None
                )
                Key(
                    name=ee_states
                    shape=[6]
                    modality=low_dim
                    randomizer=ModuleList(
                      (0): None
                    )
                    net=None
                    sharing_from=None
                )
                Key(
                    name=eye_in_hand_rgb
                    shape=[3, 128, 128]
                    modality=rgb
                    randomizer=ModuleList(
                      (0): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)
                    )
                    net=VisualCoreLanguageConditioned(
                      input_shape=[3, 116, 116]
                      output_shape=[64]
                      backbone_net=ResNet18ConvFiLM(input_channel=3, input_coord_conv=False)
                      pool_net=SpatialSoftmax(num_kp=32, temperature=1.0, noise=0.0)
                    )
                    sharing_from=None
                )
                Key(
                    name=gripper_states
                    shape=[2]
                    modality=low_dim
                    randomizer=ModuleList(
                      (0): None
                    )
                    net=None
                    sharing_from=None
                )
                Key(
                    name=joint_states
                    shape=[7]
                    modality=low_dim
                    randomizer=ModuleList(
                      (0): None
                    )
                    net=None
                    sharing_from=None
                )
                Key(
                    name=lang_emb
                    shape=[768]
                    modality=low_dim
                    randomizer=ModuleList(
                      (0): None
                    )
                    net=None
                    sharing_from=None
                )
                output_shape=[917]
            )
        )
  
        transformer=GPT_Backbone(
          (activation): GELU(approximate='none')
          (nets): ModuleDict(
            (transformer): Sequential(
              (0): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (1): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (2): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (3): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (4): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (5): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (6): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
              (7): SelfAttentionBlock(
                (nets): ModuleDict(
                  (attention): CausalSelfAttention(
                    (nets): ModuleDict(
                      (qkv): Linear(in_features=256, out_features=768, bias=False)
                      (attn_dropout): Dropout(p=0.1, inplace=False)
                      (output_dropout): Dropout(p=0.1, inplace=False)
                      (output): Linear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (mlp): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): GELU(approximate='none')
                    (2): Linear(in_features=1024, out_features=256, bias=True)
                    (3): Dropout(p=0.1, inplace=False)
                  )
                  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (output_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
  
        decoder=ObservationDecoder(
            Key(
                name=mean
                shape=(5, 7)
                modality=low_dim
                net=(Linear(in_features=256, out_features=35, bias=True))
            )
            Key(
                name=scale
                shape=(5, 7)
                modality=low_dim
                net=(Linear(in_features=256, out_features=35, bias=True))
            )
            Key(
                name=logits
                shape=(5,)
                modality=low_dim
                net=(Linear(in_features=256, out_features=5, bias=True))
            )
        )
    )
  )
)

getting language embeddings...
  0%|          | 0/2 [00:00<?, ?it/s]['put the right moka pot on the stove', 'turn off the stove', 'turn off the stove', 'turn off the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'turn off the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'turn off the stove', 'turn off the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'turn off the stove', 'turn off the stove', 'turn off the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove']
 50%|#####     | 1/2 [00:00<00:00,  3.77it/s]['put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'turn off the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'turn off the stove', 'put the right moka pot on the stove', 'turn off the stove', 'turn off the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put the right moka pot on the stove', 'put both moka pots on the stove', 'put both moka pots on the stove', 'put both moka pots on the stove']
100%|##########| 2/2 [00:00<00:00,  6.91it/s]

============= Training Dataset =============
SequenceDataset (
	path=/home/jacob/projects/strap_project/STRAP/data/retrieval_results/put_both_moka_pots_retrieved_dataset.hdf5
	obs_keys=('agentview_rgb', 'ee_ori', 'ee_pos', 'ee_states', 'eye_in_hand_rgb', 'gripper_states', 'joint_states')
	seq_length=5
	filter_key=none
	frame_stack=5
	pad_seq_length=False
	pad_frame_stack=False
	goal_mode=none
	cache_mode=none
	num_demos=103
	num_sequences=7101
)

SequenceDataset: normalizing actions...
  0%|          | 0/102 [00:00<?, ?it/s]100%|##########| 102/102 [00:00<00:00, 13194.11it/s]
**************************************************
Warnings generated by robomimic have been duplicated here (from above) for convenience. Please check them carefully.
[33mROBOMIMIC WARNING(
    No private macro file found!
    It is recommended to use a private macro file
    To setup, run: python /home/jacob/projects/strap_project/robomimic_ret/robomimic/scripts/setup_macros.py
)[0m
**************************************************


Epoch 0 Memory Usage: 1179 MB

  0%|          | 0/2 [00:00<?, ?it/s]ObservationKeyToModalityDict: pad_mask not found, adding pad_mask to mapping with assumed low_dim modality!
 50%|#####     | 1/2 [00:00<00:00,  1.27it/s]100%|##########| 2/2 [00:00<00:00,  2.29it/s]
Train Epoch 1
{
    "Log_Likelihood": -6.728669166564941,
    "Loss": 6.728669166564941,
    "Optimizer/policy0_lr": 1.5000000000000002e-08,
    "Policy_Grad_Norms": 1949.5746046599006,
    "Time_Data_Loading": 0.001585698127746582,
    "Time_Epoch": 0.015145158767700196,
    "Time_Log_Info": 3.94741694132487e-05,
    "Time_Process_Batch": 0.000987092653910319,
    "Time_Train_Batch": 0.011760373910268148
}

Epoch 1 Memory Usage: 2117 MB

  0%|          | 0/2 [00:00<?, ?it/s] 50%|#####     | 1/2 [00:00<00:00,  6.21it/s]100%|##########| 2/2 [00:00<00:00,  8.51it/s]
Train Epoch 2
{
    "Log_Likelihood": -6.472835063934326,
    "Loss": 6.472835063934326,
    "Optimizer/policy0_lr": 3.4999999999999996e-08,
    "Policy_Grad_Norms": 1703.3464333145603,
    "Time_Data_Loading": 0.0013711214065551757,
    "Time_Epoch": 0.004860234260559082,
    "Time_Log_Info": 6.274779637654622e-05,
    "Time_Process_Batch": 0.0007227460543314616,
    "Time_Train_Batch": 0.00175629456837972
}
/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/robosuite/__init__.py:7: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  ROBOSUITE_DEFAULT_LOGGER.warn("No private macro file found!")
[robosuite WARNING] No private macro file found! (__init__.py:7)
[robosuite WARNING] It is recommended to use a private macro file (__init__.py:8)
[robosuite WARNING] To setup, run: python /home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (__init__.py:9)
/home/jacob/projects/strap_project/LIBERO/libero/libero
FrameStackWrapper(
    num_frames=5
    env=KITCHEN_SCENE8_put_both_moka_pots_on_the_stove
)
video writes to /home/jacob/projects/strap_project/robomimic_ret/expdata/libero/im/bc_xfmr/02-05-run/123/02-05-run/videos/KITCHEN_SCENE8_put_both_moka_pots_on_the_stove_epoch_2.mp4
rollout: env=KITCHEN_SCENE8_put_both_moka_pots_on_the_stove, horizon=30, use_goals=False, num_episodes=2
  0%|          | 0/2 [00:00<?, ?it/s]put both moka pots on the stove
 50%|#####     | 1/2 [00:03<00:03,  3.38s/it]put both moka pots on the stove
100%|##########| 2/2 [00:04<00:00,  1.90s/it]100%|##########| 2/2 [00:04<00:00,  2.13s/it]

Epoch 2 Rollouts took 2.1249696016311646s (avg) with results:
Env: KITCHEN_SCENE8_put_both_moka_pots_on_the_stove
{
    "Horizon": 30.0,
    "Return": 0.0,
    "Success_Rate": 0.0,
    "Time_Episode": 0.07083232005437215,
    "time": 2.1249696016311646
}

Epoch 2 Memory Usage: 3232 MB

finished run successfully!
Exception ignored in: <function MjRenderContext.__del__ at 0x776ab8878b80>
Traceback (most recent call last):
  File "/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/robosuite/utils/binding_utils.py", line 199, in __del__
    self.gl_ctx.free()
  File "/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/robosuite/renderers/context/egl_context.py", line 149, in free
    EGL.eglMakeCurrent(EGL_DISPLAY, EGL.EGL_NO_SURFACE, EGL.EGL_NO_SURFACE, EGL.EGL_NO_CONTEXT)
  File "/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/OpenGL/error.py", line 230, in glCheckError
    raise self._errorClass(
OpenGL.raw.EGL._errors.EGLError: EGLError(
	err = EGL_NOT_INITIALIZED,
	baseOperation = eglMakeCurrent,
	cArguments = (
		<OpenGL._opaque.EGLDisplay_pointer object at 0x776ab83dd9c0>,
		<OpenGL._opaque.EGLSurface_pointer object at 0x7769315281c0>,
		<OpenGL._opaque.EGLSurface_pointer object at 0x7769315281c0>,
		<OpenGL._opaque.EGLContext_pointer object at 0x7769315283c0>,
	),
	result = 0
)
Exception ignored in: <function EGLGLContext.__del__ at 0x776ab88789d0>
Traceback (most recent call last):
  File "/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/robosuite/renderers/context/egl_context.py", line 155, in __del__
    self.free()
  File "/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/robosuite/renderers/context/egl_context.py", line 149, in free
    EGL.eglMakeCurrent(EGL_DISPLAY, EGL.EGL_NO_SURFACE, EGL.EGL_NO_SURFACE, EGL.EGL_NO_CONTEXT)
  File "/home/jacob/miniforge3/envs/strap2/lib/python3.10/site-packages/OpenGL/error.py", line 230, in glCheckError
    raise self._errorClass(
OpenGL.raw.EGL._errors.EGLError: EGLError(
	err = EGL_NOT_INITIALIZED,
	baseOperation = eglMakeCurrent,
	cArguments = (
		<OpenGL._opaque.EGLDisplay_pointer object at 0x776ab83dd9c0>,
		<OpenGL._opaque.EGLSurface_pointer object at 0x7769315281c0>,
		<OpenGL._opaque.EGLSurface_pointer object at 0x7769315281c0>,
		<OpenGL._opaque.EGLContext_pointer object at 0x7769315283c0>,
	),
	result = 0
)
